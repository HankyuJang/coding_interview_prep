{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "[link](https://www.youtube.com/watch?v=RIg3iuen7MY&ab_channel=DataInterviewPro)\n",
    "\n",
    "- Straight-line: Relation btw x and y is linear\n",
    "- Non-linear relation: should not use linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g., predict demand or multiple independent variables based on price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "- $y$: dependent variable\n",
    "- $\\hat{y}$: estimation of $y$\n",
    "- $x$: independent variable\n",
    "- $\\beta_0$: intercept\n",
    "- $\\beta_1$: slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$\n",
    "\n",
    "- $x_1, \\dots , x_n$: independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where do betas come from? Get betas to minimize error! E.g., MSE\n",
    "\n",
    "## Ordinary least squares (OLS)\n",
    "\n",
    "the method used in linear regression to estimate betas to get minimum square distance btw values.\n",
    "\n",
    "__Objective__: find optimal betas such that the error is minimal.\n",
    "\n",
    "How do we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "- start w/ random guess of betas\n",
    "- Compute the MSE\n",
    "- Compute the gradients and update betas\n",
    "- Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient: derivative of error w.r.t. a particular parameter\n",
    "\n",
    "$$gradient_{\\beta_0} = \\frac{\\partial Error}{\\partial \\beta_0}$$\n",
    "\n",
    "We can decompose the gradient with a chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain rule\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the chain rule...\n",
    "\n",
    "$$gradient_{\\beta_0} = \\frac{\\partial Error}{\\partial \\beta_0}$$\n",
    "$$ = \\frac{\\partial Error}{\\partial y} \\frac{\\partial y}{\\partial \\beta_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use mean squared error (MSE),\n",
    "\n",
    "$$Error = \\frac{\\sum{(y_i - \\hat{y}_i)}^2}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking derivative of Error w.r.t. y, we get\n",
    "\n",
    "$$ gradient_{\\beta_0} = mean(\\frac{\\partial Error}{\\partial y} \\frac{\\partial y}{\\partial \\beta_0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= mean_j (2 \\cdot (y_j - \\hat{y}_j) \\cdot 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ gradient_{\\beta_i} = mean(\\frac{\\partial Error}{\\partial y} \\frac{\\partial y}{\\partial \\beta_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= mean_j (2 \\cdot (y_j - \\hat{y}_j) \\cdot x_{j, i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how beta is updated\n",
    "\n",
    "- Derivative of a gradient (e.g., $\\beta_i$) would be negative if $\\hat{y}$ is over estimated. \n",
    "- Then, the derivative would have a negative value.\n",
    "- In this case, you need to make $\\beta_i$ smaller.\n",
    "- Hence, you add the __negative gradient__ to beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "- $m$ data points\n",
    "- $n$ columns\n",
    "- $i$ iteration of weight update\n",
    "\n",
    "To compute gradient, for each iteration, it takes $O(mn)$\n",
    "\n",
    "To update betas, it takes $O(n)$\n",
    "\n",
    "Bottlenect is on computing gradients. Total time complexity of the algorithm is then $O(mni)$\n",
    "\n",
    "We save betas, so, space complexity: $O(n)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Dataset $X$ has $m$ data points and $n$ columns\n",
    "\n",
    "- $n+1$ total betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
